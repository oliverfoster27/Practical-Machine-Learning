{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C7 Exercises.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oliverfoster27/Practical-Machine-Learning/blob/master/Week%207/C7_Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ywZ2fsAxP6ON",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l94hdaVKQTtF",
        "colab_type": "code",
        "outputId": "9b80b373-ec95-4bb4-f6c4-3e68b219dacc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data('/tmp/mnist.npz')\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "X_train = X_train.reshape(-1, 28*28)\n",
        "X_test = X_test.reshape(-1, 28*28)\n",
        "X_train.shape, X_test.shape, y_train, y_test"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 784),\n",
              " (10000, 784),\n",
              " array([5, 0, 4, ..., 5, 6, 8], dtype=uint8),\n",
              " array([7, 2, 1, ..., 4, 5, 6], dtype=uint8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "Xj0EkNVnxzhl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255\n",
        "X_test = X_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sORyfkzHQYRQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "Make a dense neural network with 95%+ accuracy on Mnist that has the smallest number of neurons possible by experimenting with Dropout, and Batch Norm"
      ]
    },
    {
      "metadata": {
        "id": "5pISPLb1-XAx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Strategy:\n",
        "- Decide on a basic layer architecutre (one hidden dense layer with dropout and batch normalization)\n",
        "- Iterate starting with 10 neurons to 100 on the hidden layer and find the optimal learning rate & p value for dropout\n",
        "- If early stopping happens during training it indicates that the network may not be powerful enough. At this point increase the size of the dense layer\n",
        "- When validation accuracy reaches 95% return that network's architecture"
      ]
    },
    {
      "metadata": {
        "id": "t7S3iZx1QWrB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "# Grids to iterate through\n",
        "learning_grid = [10e-6, 10e-5, 10e-4]\n",
        "input_dense_grid = np.linspace(10, 100, 19).astype(int)\n",
        "p_grid = [0.2, 0.4, 0.6]\n",
        "\n",
        "def find_minimal_network(features, output, val_thresh=0.95):\n",
        "\n",
        "  data = []\n",
        "\n",
        "  for input_dense in input_dense_grid:\n",
        "    \n",
        "    stopped_early = False\n",
        "    \n",
        "    for learning_rate in learning_grid:\n",
        "      \n",
        "      if stopped_early:\n",
        "        break\n",
        "      \n",
        "      for p in p_grid:\n",
        "\n",
        "        K.clear_session()\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        # Baseline architecture\n",
        "        model.add(Dense(input_dense, activation='relu', input_shape=(784,)))\n",
        "        model.add(Dropout(p))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "        callback_list = [EarlyStopping(monitor='val_acc', mode='max',\n",
        "                                       verbose=0, patience=5)]\n",
        "\n",
        "        optimizer = Adam(lr=learning_rate)\n",
        "        model.compile(optimizer=optimizer,\n",
        "                     loss='sparse_categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "        print(\"\\nNumber of Dense Nodes: {}\".format(input_dense))\n",
        "        print(\"Learning Rate: {}\".format(learning_rate))\n",
        "        print(\"Dropout P: {}\".format(p))\n",
        "\n",
        "        h = model.fit(features, output, epochs=100, validation_split=0.3,\n",
        "                     callbacks=callback_list, verbose=0)\n",
        "\n",
        "        # If we stop early our learning rate doesn't need to increase\n",
        "        # so once we're done iterating through p increase the layer size\n",
        "        stopping_interval = callback_list[0].stopped_epoch\n",
        "        if stopping_interval > 0:\n",
        "          stopped_early = True\n",
        "\n",
        "        print(\"Early Stopping: {}\".format(stopping_interval))\n",
        "        print(\"Trained Validation Accuracy: {}\".format(h.history['val_acc'][-1]))\n",
        "\n",
        "        if h.history['val_acc'][-1] >= val_thresh:\n",
        "          res={\n",
        "            'Input Dense Layer Size': input_dense,\n",
        "            'Learning Rate': learning_rate,\n",
        "            'Dropout Proportion': p,\n",
        "            'Validation Accuracy': h.history['acc'][-1]\n",
        "          }\n",
        "          # If we've satisfied 95% accuracy leave the learning rate grid\n",
        "          return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r4ddfMEDzDSx",
        "colab_type": "code",
        "outputId": "2bc87ea8-a28d-4259-cf9a-15aeb32e39a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1822
        }
      },
      "cell_type": "code",
      "source": [
        "optimal_network = find_minimal_network(X_train, y_train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "\n",
            "Number of Dense Nodes: 10\n",
            "Learning Rate: 1e-05\n",
            "Dropout P: 0.2\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Early Stopping: 0\n",
            "Trained Validation Accuracy: 0.8922777777777777\n",
            "\n",
            "Number of Dense Nodes: 10\n",
            "Learning Rate: 1e-05\n",
            "Dropout P: 0.4\n",
            "Early Stopping: 0\n",
            "Trained Validation Accuracy: 0.8823888888888889\n",
            "\n",
            "Number of Dense Nodes: 10\n",
            "Learning Rate: 1e-05\n",
            "Dropout P: 0.6\n",
            "Early Stopping: 0\n",
            "Trained Validation Accuracy: 0.8424444444444444\n",
            "\n",
            "Number of Dense Nodes: 10\n",
            "Learning Rate: 0.0001\n",
            "Dropout P: 0.2\n",
            "Early Stopping: 30\n",
            "Trained Validation Accuracy: 0.9103888888888889\n",
            "\n",
            "Number of Dense Nodes: 10\n",
            "Learning Rate: 0.0001\n",
            "Dropout P: 0.4\n",
            "Early Stopping: 30\n",
            "Trained Validation Accuracy: 0.8892222222222222\n",
            "\n",
            "Number of Dense Nodes: 10\n",
            "Learning Rate: 0.0001\n",
            "Dropout P: 0.6\n",
            "Early Stopping: 31\n",
            "Trained Validation Accuracy: 0.8757777777777778\n",
            "\n",
            "Number of Dense Nodes: 15\n",
            "Learning Rate: 1e-05\n",
            "Dropout P: 0.2\n",
            "Early Stopping: 78\n",
            "Trained Validation Accuracy: 0.9126111111111112\n",
            "\n",
            "Number of Dense Nodes: 15\n",
            "Learning Rate: 1e-05\n",
            "Dropout P: 0.4\n",
            "Early Stopping: 0\n",
            "Trained Validation Accuracy: 0.9080555555555555\n",
            "\n",
            "Number of Dense Nodes: 15\n",
            "Learning Rate: 1e-05\n",
            "Dropout P: 0.6\n",
            "Early Stopping: 0\n",
            "Trained Validation Accuracy: 0.8847222222222222\n",
            "\n",
            "Number of Dense Nodes: 20\n",
            "Learning Rate: 1e-05\n",
            "Dropout P: 0.2\n",
            "Early Stopping: 88\n",
            "Trained Validation Accuracy: 0.9285555555555556\n",
            "\n",
            "Number of Dense Nodes: 20\n",
            "Learning Rate: 1e-05\n",
            "Dropout P: 0.4\n",
            "Early Stopping: 0\n",
            "Trained Validation Accuracy: 0.9137222222222222\n",
            "\n",
            "Number of Dense Nodes: 20\n",
            "Learning Rate: 1e-05\n",
            "Dropout P: 0.6\n",
            "Early Stopping: 0\n",
            "Trained Validation Accuracy: 0.8990555555555556\n",
            "\n",
            "Number of Dense Nodes: 25\n",
            "Learning Rate: 1e-05\n",
            "Dropout P: 0.2\n",
            "Early Stopping: 0\n",
            "Trained Validation Accuracy: 0.9355555555555556\n",
            "\n",
            "Number of Dense Nodes: 25\n",
            "Learning Rate: 1e-05\n",
            "Dropout P: 0.4\n",
            "Early Stopping: 0\n",
            "Trained Validation Accuracy: 0.9193888888888889\n",
            "\n",
            "Number of Dense Nodes: 25\n",
            "Learning Rate: 1e-05\n",
            "Dropout P: 0.6\n",
            "Early Stopping: 0\n",
            "Trained Validation Accuracy: 0.9048888888888889\n",
            "\n",
            "Number of Dense Nodes: 25\n",
            "Learning Rate: 0.0001\n",
            "Dropout P: 0.2\n",
            "Early Stopping: 59\n",
            "Trained Validation Accuracy: 0.9515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9F_Xz0aq0XGZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train network on minimum architecture"
      ]
    },
    {
      "metadata": {
        "id": "LhlvmVKd0Urx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2720
        },
        "outputId": "656fc19a-b732-4b36-a466-adc2e74dd739"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "input_dense = optimal_network['Input Dense Layer Size']\n",
        "learning_rate = optimal_network['Learning Rate']\n",
        "p = optimal_network['Dropout Proportion']\n",
        "\n",
        "print(\"\\nNumber of Dense Nodes: {}\".format(input_dense))\n",
        "print(\"Learning Rate: {}\".format(learning_rate))\n",
        "print(\"Dropout P: {}\\n\".format(p))\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(input_dense, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(p))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "callback_list = [EarlyStopping(monitor='val_acc', mode='max',\n",
        "                               verbose=0, patience=5)]\n",
        "\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "model.compile(optimizer=optimizer,\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "h = model.fit(X_train, y_train, epochs=100, validation_split=0.3,\n",
        "             callbacks=callback_list, verbose=1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of Dense Nodes: 25\n",
            "Learning Rate: 0.0001\n",
            "Dropout P: 0.2\n",
            "\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 25)                19625     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 25)                100       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                260       \n",
            "=================================================================\n",
            "Total params: 19,985\n",
            "Trainable params: 19,935\n",
            "Non-trainable params: 50\n",
            "_________________________________________________________________\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 7s 161us/step - loss: 1.3284 - acc: 0.5979 - val_loss: 0.7249 - val_acc: 0.8461\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 7s 161us/step - loss: 0.7479 - acc: 0.8036 - val_loss: 0.4941 - val_acc: 0.8882\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 7s 155us/step - loss: 0.5929 - acc: 0.8377 - val_loss: 0.4051 - val_acc: 0.9016\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 6s 151us/step - loss: 0.5049 - acc: 0.8593 - val_loss: 0.3496 - val_acc: 0.9085\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 6s 152us/step - loss: 0.4636 - acc: 0.8674 - val_loss: 0.3149 - val_acc: 0.9147\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 6s 151us/step - loss: 0.4289 - acc: 0.8758 - val_loss: 0.2948 - val_acc: 0.9199\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 6s 148us/step - loss: 0.4036 - acc: 0.8849 - val_loss: 0.2824 - val_acc: 0.9231\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 6s 151us/step - loss: 0.3875 - acc: 0.8862 - val_loss: 0.2693 - val_acc: 0.9252\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 7s 163us/step - loss: 0.3661 - acc: 0.8935 - val_loss: 0.2606 - val_acc: 0.9284\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.3585 - acc: 0.8940 - val_loss: 0.2524 - val_acc: 0.9291\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 6s 151us/step - loss: 0.3498 - acc: 0.8963 - val_loss: 0.2446 - val_acc: 0.9332\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.3389 - acc: 0.8998 - val_loss: 0.2383 - val_acc: 0.9342\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.3332 - acc: 0.9011 - val_loss: 0.2298 - val_acc: 0.9372\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 7s 157us/step - loss: 0.3254 - acc: 0.9039 - val_loss: 0.2284 - val_acc: 0.9373\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 7s 160us/step - loss: 0.3165 - acc: 0.9041 - val_loss: 0.2240 - val_acc: 0.9387\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.3131 - acc: 0.9065 - val_loss: 0.2226 - val_acc: 0.9388\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 6s 151us/step - loss: 0.3094 - acc: 0.9074 - val_loss: 0.2195 - val_acc: 0.9390\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.3035 - acc: 0.9082 - val_loss: 0.2157 - val_acc: 0.9407\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.3002 - acc: 0.9093 - val_loss: 0.2136 - val_acc: 0.9408\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.2998 - acc: 0.9091 - val_loss: 0.2105 - val_acc: 0.9426\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.2912 - acc: 0.9112 - val_loss: 0.2092 - val_acc: 0.9422\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.2884 - acc: 0.9126 - val_loss: 0.2087 - val_acc: 0.9430\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 6s 148us/step - loss: 0.2851 - acc: 0.9141 - val_loss: 0.2035 - val_acc: 0.9442\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.2798 - acc: 0.9156 - val_loss: 0.2020 - val_acc: 0.9450\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.2775 - acc: 0.9157 - val_loss: 0.2046 - val_acc: 0.9436\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.2704 - acc: 0.9171 - val_loss: 0.2000 - val_acc: 0.9457\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 7s 160us/step - loss: 0.2707 - acc: 0.9175 - val_loss: 0.1968 - val_acc: 0.9457\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 7s 156us/step - loss: 0.2664 - acc: 0.9186 - val_loss: 0.1981 - val_acc: 0.9456\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.2658 - acc: 0.9172 - val_loss: 0.1945 - val_acc: 0.9468\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 6s 148us/step - loss: 0.2631 - acc: 0.9194 - val_loss: 0.1936 - val_acc: 0.9475\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.2646 - acc: 0.9206 - val_loss: 0.1959 - val_acc: 0.9467\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 7s 170us/step - loss: 0.2598 - acc: 0.9205 - val_loss: 0.1926 - val_acc: 0.9481\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 7s 161us/step - loss: 0.2553 - acc: 0.9229 - val_loss: 0.1945 - val_acc: 0.9476\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.2570 - acc: 0.9218 - val_loss: 0.1942 - val_acc: 0.9468\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.2499 - acc: 0.9224 - val_loss: 0.1909 - val_acc: 0.9476\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.2548 - acc: 0.9210 - val_loss: 0.1905 - val_acc: 0.9483\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 6s 148us/step - loss: 0.2490 - acc: 0.9228 - val_loss: 0.1913 - val_acc: 0.9475\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 6s 151us/step - loss: 0.2485 - acc: 0.9235 - val_loss: 0.1889 - val_acc: 0.9481\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 7s 155us/step - loss: 0.2502 - acc: 0.9224 - val_loss: 0.1872 - val_acc: 0.9478\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 7s 162us/step - loss: 0.2457 - acc: 0.9241 - val_loss: 0.1876 - val_acc: 0.9474\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 6s 151us/step - loss: 0.2446 - acc: 0.9251 - val_loss: 0.1864 - val_acc: 0.9490\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.2414 - acc: 0.9271 - val_loss: 0.1868 - val_acc: 0.9485\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.2415 - acc: 0.9256 - val_loss: 0.1857 - val_acc: 0.9485\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 6s 151us/step - loss: 0.2386 - acc: 0.9264 - val_loss: 0.1848 - val_acc: 0.9482\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.2363 - acc: 0.9271 - val_loss: 0.1838 - val_acc: 0.9491\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.2331 - acc: 0.9284 - val_loss: 0.1849 - val_acc: 0.9481\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.2382 - acc: 0.9275 - val_loss: 0.1866 - val_acc: 0.9484\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 6s 151us/step - loss: 0.2350 - acc: 0.9282 - val_loss: 0.1858 - val_acc: 0.9480\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 6s 151us/step - loss: 0.2357 - acc: 0.9254 - val_loss: 0.1861 - val_acc: 0.9490\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.2352 - acc: 0.9275 - val_loss: 0.1836 - val_acc: 0.9497\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 6s 151us/step - loss: 0.2301 - acc: 0.9283 - val_loss: 0.1857 - val_acc: 0.9491\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 7s 159us/step - loss: 0.2339 - acc: 0.9275 - val_loss: 0.1845 - val_acc: 0.9495\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 7s 159us/step - loss: 0.2274 - acc: 0.9282 - val_loss: 0.1813 - val_acc: 0.9491\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.2320 - acc: 0.9276 - val_loss: 0.1851 - val_acc: 0.9494\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.2287 - acc: 0.9277 - val_loss: 0.1803 - val_acc: 0.9502\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 6s 153us/step - loss: 0.2284 - acc: 0.9285 - val_loss: 0.1829 - val_acc: 0.9494\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 7s 162us/step - loss: 0.2290 - acc: 0.9285 - val_loss: 0.1816 - val_acc: 0.9501\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.2262 - acc: 0.9282 - val_loss: 0.1837 - val_acc: 0.9501\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.2281 - acc: 0.9276 - val_loss: 0.1806 - val_acc: 0.9507\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.2222 - acc: 0.9318 - val_loss: 0.1803 - val_acc: 0.9500\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 6s 148us/step - loss: 0.2198 - acc: 0.9300 - val_loss: 0.1834 - val_acc: 0.9501\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.2250 - acc: 0.9293 - val_loss: 0.1832 - val_acc: 0.9504\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 6s 148us/step - loss: 0.2249 - acc: 0.9300 - val_loss: 0.1805 - val_acc: 0.9499\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 6s 150us/step - loss: 0.2233 - acc: 0.9310 - val_loss: 0.1825 - val_acc: 0.9516\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 7s 161us/step - loss: 0.2194 - acc: 0.9307 - val_loss: 0.1832 - val_acc: 0.9498\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 6s 153us/step - loss: 0.2210 - acc: 0.9309 - val_loss: 0.1824 - val_acc: 0.9498\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 6s 148us/step - loss: 0.2151 - acc: 0.9321 - val_loss: 0.1813 - val_acc: 0.9508\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 6s 147us/step - loss: 0.2127 - acc: 0.9327 - val_loss: 0.1813 - val_acc: 0.9510\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 6s 147us/step - loss: 0.2199 - acc: 0.9298 - val_loss: 0.1796 - val_acc: 0.9516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U1riebZa2x1Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "520f85f4-2d32-4e30-aaaa-b4f86b685f2c"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(h.history['acc'], label='Training Accuracy')\n",
        "plt.plot(h.history['val_acc'], label='Validation Accuracy')\n",
        "plt.legend()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f942e27a320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl81PW97/HXZybLZN8hQIAERfYt\nRKzihopSe68UtyJqXWo59VTb6tF7aOtVSzd7j6fH1sOxtb1Q9bYg1aNyKtZq1aPWqgTZJIBsQbJA\n9n2Z7Xv/+E7CJCRkQgYSJp/n4zGPzPzm9/vNZ4bwnm++v+/v+xNjDEoppYYHx2AXoJRS6vTR0FdK\nqWFEQ18ppYYRDX2llBpGNPSVUmoY0dBXSqlhRENfKaWGEQ19pZQaRjT0lVJqGIka7AK6y8zMNLm5\nuYNdhlJKnVE2b95cZYzJ6mu9IRf6ubm5FBYWDnYZSil1RhGRQ6Gsp907Sik1jGjoK6XUMKKhr5RS\nw4iGvlJKDSMa+kopNYyEFPoiskhE9ojIPhFZ0cPz40XkryKyXUTeEZGcoOd8IrI1cNsQzuKVUkr1\nT59DNkXECawCFgIlwCYR2WCMKQpa7XHgWWPMMyJyGfBT4NbAc63GmNlhrlsppdRJCGWc/jxgnzHm\nAICIrAMWA8GhPxW4P3D/beDlcBaplFL9ZgyIhG9/zdVQ9gkc/RRSx0HOPEjJCe01fB5oqwef2973\ne8Hbbpe11h67xaXCnFvCV3MPQgn9McDhoMclwHnd1tkGXAv8AlgCJIlIhjGmGnCJSCHgBR4zxugX\nglJ98brBEQWOARx2MwbaGyEmcWD7AWhvguYKG3wtVdBcaZe7UiEuzd5ik8DbZtd1B27itMs7btHx\n4GkJPN9s122tgaYKu/+mSmhvgPgMSBwBCSMgMcsGZXPVsdduawiErYA47P32Rvtcc5W9eVshPjOw\nnyz70xltPxfjtz/9nkC9zbYmTwtEucCVEril2uVln0Dd58d/LkmjIOdcyJxo6+jg90JDud2m7nNo\nLLOv2ZfRc4ZE6IfiAeDfReR24F2gFPAFnhtvjCkVkQnAWyKywxizP3hjEVkOLAcYN25cmEpSw467\nxQZHWwMkZNrAcEZ1fb5qD1TsgtpDEBMf9J87xQZNwgi7rTP62HbtjTaUGo9A01EbLB0h5W622ySP\ngqTRkJQNxgetdcdab54WiIqFqDiIdtmfMfEQkwQxCRCbCJ5WKN8GZVugbCtU7rYhEZt8rL6YeKBb\nqzIq1u4jOs4Gqt8LDaVQXwoNZTb4nDGQMta2TtPG2316WuxrupttizMq1tYRk2j352mDukPHQqut\n7tT/+zlj7GcZmwSHP7YB3z0oxWn/fVwp9nFngPvtdgmZkH6WDflolw3/jn+rmv3g99Hli8IRdex9\nJ460n2NHC7zmoP3pcMLofDj3Lvtz5DT72RzeBCUf21p3/6lbnQ77hZA6DvIusj/jA79XzmhwBH7G\npXb74kw+5R+zGGNOvILI+cCjxpirAo+/C2CM+Wkv6ycCu40xOT089zvgT8aYF3p7vYKCAqPTMAxD\nfj9U7wNfe7cnAv8xHU578/uhthiqPoPqvVC1F+pLbBC7m7pt6rAhkpRtg7vmAHDi3/dOcek2RJqr\nwNN8/PMd4RMdb0Olp3WC30OorxufaVt7o2ba991Wf+zm7v4axgaUp8V+oXla7XtOGQPJoyF5jG3d\ntlTb4K49ZMPK3Wzrjo63XyRRsXY/nS3eRnDG2i+I1HH2lpJjQzE+ExIy7E9x2C+Dji+39kYbmh1f\nHDEJNozbG4/dPC2B1+1YJxHi021Iu1K6dpX4fdBSYwPbGWO/lF2pA/+rJUKJyGZjTEFf64XS0t8E\nTBSRPGwLfimwrNuLZQI1xhg/8F1gdWB5GtBijGkPrDMf+D/9eifqzOH321Zm9b7An7Tl9tZQbkM5\ndSyMnB64TbOt0gPvwMH/hoPv2uDoD1cKZEy0IZk40nYDJI60raWWKtsybyizNaSOg5k3wogpMGIa\npOXarojOUK2z4dhUcawl395ogz1xpP3iSBwBiYGfcenHwqejG6Xj/Tqij7Xc4lJtd4HPY1vdnjb7\n091yrAvE3Wy/REbNtEEdzn7ok9HREAypjrGnrg6HM/Bv2uccYqof+gx9Y4xXRO4BXgecwGpjzE4R\nWQkUGmM2AJcCPxURg+3e+WZg8ynAr0XEjx0e+li3UT9qqPJ54OhO291gfIGWWaB1hgkK1MDPmgNQ\ne9AGabCELBuY8RlQvh2KXjn+tZLHwKSrYfx8cHX789b4bYvP77N1gA3wjIk2kAcSkM5E+6d9ypiT\n3wfYGlzJ9pY1qed1omLsraNbYigb7C8ddUr12b1zumn3zmng99uQLt9qD5p1hKrfa1u5JZts2Hta\n+t6XK9X2XabnQfoEyDgbMs6yLenEbBt0wdqbbJ/60R22RZl3iV1fg0apAQln944ayvw+28ddWxw0\nHMwDPq997PfYZT6P7fIo2wKlW6C9vuf9OaJtN0P+bTD2XBhTYLsn3EEjHIyxrfekUYGDi/0Qm2j3\nO/bcgb5zpdRJ0NAf6o7sgP1v2wDHBIaZ+aD+sO1+qdxj+4hD4YiCEVNh+hIYM9f2hSdk2f5kh9M+\nH+U6vnUOwMhwviul1CDR0B+KWmrg0xdhy3N2GF9PEkfaAC+40x6czDjLBnbwcLDO+zF26GJUXC+B\nrpQaLjT0B5Pfb/vVaw8eGw9dWwzF79uWffZM+OK/wLQl9iChOAIngIgOW1NKnRQN/cFgjD2Z453H\n7CndHeLS7bDGuXfAnJth1KzBq1GpCOLx+Xmz6CiuGCdzxqaSGn/q/+JtbPNwoLKZA1VNHKhspqyu\njZrmdmpaPNQ2u2lu9zL/7EyuzR/DhWdnEuU8PQ05Df3TyRjY8xq88xPbV59xNixeZc/ySx1rTwZS\nSoWN32/4045yfv6XPRRXHxuNlpeZwJyxqUwdnUxWUixZibFkJsWSnhBDS7uPyqZ2qpvaqWpyc7Sh\njfL6Vsrq2iira+VIQxsCxEQ5iI1yEhPlIMoheP0Gn9/g9ftxe/3Utng6X88hMDLZRUZiDGnxMeRl\nxONwCG/trmDDtjIyE2P58uzRLMkfw7TRp3ZYr4b+qWCMPTV71wY7jr250p7403jEjqBJy4Mv/wpm\n3NB1mgClTgFjDOX1beyraGJvRROtbi/ZKXGMTnExKjWOrKRYGlo9VDa2U9HYTmVjO+kJMVw6KQtX\ntLPHffr8hqqmdo42tHG0oZ2KxjbqWjxkJMSQneJidGoc2SkujB8qGtuoaLTr1rd6SImLJi0+hrSE\nGNLjY4iOErw+G5g+Y2jz+KhobKciaN/tHj8OEURARHA6IDbKiSvaBm9slIOUuGjSE2LISIwlIyGG\n/ZVNPP6Xz9hV3sDk7CSevnUuia4oth6uY8vndby7t4r/3FLa5+cnAiOSYhmVEseUUclcOmkEDoF2\nrw33dq8Pr98Q7XTgECHKIUQ5hZy0ePIyEzgrK4FxGfHERh3/Wbq9ft7eU8F/flLCM38v5oP91Wz8\n9kUD/Sc/IU2ccGqtg+3Pw+bfQUWRPbCaNMqOkEkdZ0fLjL8AZtyoYa/6ZIxhf2UzHx6o5u8Hqqlq\nbGdSdhJTRyUzZVQy54xMwu3zU9nYTlWTDevKxnaqm9upanRT1WRD/GBVM03t3n6/fpIrii/NGMWX\n54xhXm46pXWtvLu3kvf3VvG3fVU0tPV/nycjLT4aV7TTDlwzBoP90mn3+Gj3+vH6ez/XaFx6PE98\nZTbXzBqNw2HPBbngrEzAfr51LR6qAi36qqZ2aprdxMc4yUyKJTMhlsykGDISYomJOjVdLzFRDq6a\nls1V07KpbXZTXt/W90YDpCdnDVRtsZ1K4MA7sOfPdvjk6Dm2X376dXZculIn0Obxsa+iibK6Vnur\nb+NwTQubD9VS0WjnIspOdjEq1cXeo019BrjTIWQkxJAZ6LKYkJnA2SMSOXtEIhNHJJIQG8WR+jbK\n6lspr2ujsqmdZFc0I5JibVdHUiz7K5t4aUspf/70CC1uH4mxUZ2vOzrFxYUTM5mRk0p2souRybGM\nTHaREhdNVVM7R+rbKK+3XSIOEbKS7PMjkmJJiYumsc1LTYub2mY3Nc1ufH6Dw2FbyE6HEBvlICvJ\n7jcrKbbHFnIwr89Pu9dPXauHmiY31c3tVDe5iYlysGh6NtGnqa98sIV6cpaGfn8ZA4c/gm1r7fj5\nukN2eWI2TFpkw360XjPmTFLZ2M7beyrw+PzkZSSQm5lAdrKrs2Xo8xsaWj3UtXpIi4/u9SBgeX0r\nWz6vo6S2haomd2fLu83jY1x6PLmZCYzPiGdcejyHa1v55FAtn3xeS1FZQ5fWakyUg9EpLmbmpHL+\nWRl8YUIGuRnxiAh+v6GktpWi8nr2Hm0iLsZJVlIsmYmxnT9T46I7ax+oFreXN4qO8sG+aqaMSuLC\niVmclZWA6BnUQ46Gfri11Nig3/yMnZ43JhHyLoYJl9qpBLIm6VQCQ5AxhorGdhrbvDgdglMEhwMa\n27y8tbuCN3cdZevhOrr/N4iNcjAiOZbGNi/1rZ4uz49KcTEpO4lJ2UlkJMSwvaSeTw7VUhb0p7lt\nrdoQjolycLim5bg/3eOinczMSSF/fBozxqSQkxbH6NQ4MhJiNFRVv+k0DOHS1gB//QF88qwdO59z\nLlzz73bsvHbdnFbGGJrdPhJinD2GYpvHx+ZDtXx0sIYDlU0crGrmYFUzLW5fD3uzZuakcP8V53DF\n1JEkx0VzqKqZg9XNFFc1U9Fouz06Wvep8dFUNraz+0gju4808rd9VXh8hjGpcczNTefr41LJH5fG\nhKwEEmOjjqux1e3j85oWPq9pYVSKi8nZSadtmJ5SHTT0T2Tvm/Bf37LT5c693V5EYeS0wa4qIrW4\nvRysaqa+xUNDm5emdi+NbR6ONrTzeU0zxVUtHKpuptntIy0+msnZyUwelcTk7CQaWr28u7eSjw/W\n0O714xAYm25HTszLSycvM4GUuGj8xuDz22F80VHC+RMyyU5xdaljTGocF5ydGVLNHp+fhlYPGYmx\nIa0fF+Ps/AtBqcGiod+T1lp4/fuw9feQNRlufBZy+vyrSfXA4/Ozr6KJz4420uK2Q9v8foPXb6hu\nauezo418drSJw7Utx3WxAEQ7hbFp8YzPiGdeXjojkmP5vLqFXUcaWffxYVo9thV/9ohEbpo3josm\nZnLehAwSY0/9r3a00xFy4Cs1VGjod1f8PrzwNTu2/qJ/gkv+2V5ZSPWpsc3D7iONFJU1UFTWwM7y\nej470oTb1/O1QaMcwoSsBGbmpHD93BzOHpFIekIMSa4okmKjSXRFkeyK6rULxO83fF7TgivaeVyL\nXSnVMw39DsbA356Av66019hcts4OvVTH8fj8FFc1syfQSv/sSCO7jjRwKOiMx/SEGKaNTuaO+blM\nHW3HlSe7onE4IMrhwOkQ4mOcAxpO53AIuZkJ4XhLSg0bGvpgT6p6+W7Ys9EeoL3mSZ0SIUi718eW\nz+v4YF8V7++rYkdpPR6f7YtxCIzPSGDa6GRumJvD1NHJTB2VwsjkWB2BotQQpKF/ZAc8f4u9uPai\nx+C8b0T80Ms2j4+KhnaONtqTgIqr7UHS4uoWqpvaO+cUiY1y4BDYVd5Iq8eHQ2DW2FTuvDCPydlJ\nnDMyibOyEns9VV8pNfQM79D/7C/wx9vtdUtv3wjjzhvsisLC5zdsL6ljf2Uz5XWtlNW3UlrXxpH6\nVo42tFPf6umyvkNgdGocuRkJ5GXE4/GbwJwifjxeP185dyzzz87kvAnpJLuiB+ldKaXCIaTQF5FF\nwC+wF0b/rTHmsW7PjwdWA1lADXCLMaYk8NxtwEOBVX9kjHkmTLUPzKbfwsYHIXsGLFtvL/93Bqtv\n8fDfeyt5e3cF7+yp6DLDX2ZiTGeof2FCRucp8SOSXYxNiyMnLf6UzS2ilBpa+gx9EXECq4CFQAmw\nSUQ2GGOKglZ7HHjWGPOMiFwG/BS4VUTSgUeAAsAAmwPb1ob7jYTM74c3H4YPnoSJV8H1q8+Yk6xq\nmt28ur2MP20vp7y+DbfXj9tnZ/prdnsxxk5OdemkESyYPIKZY1LITnFp94tSqlMoLf15wD5jzAEA\nEVkHLAaCQ38qcH/g/tvAy4H7VwFvGGNqAtu+ASwC1g689JPg88KLX4Oil+2JVot+NqRnu/T7DaV1\nrWw+VMsrW0t5b28VXr/hnJGJzB2fRrRTiIlyEON0khIXzYUTM5g9Ng1nmOZdUUpFnlASbwxwOOhx\nCdC983sbcC22C2gJkCQiGb1sO+akqx2owv9rA/+KR2H+d4bcAdumdi8vbSllc3EN+yqb2FfRRJvH\njnEfneLirosmsHj2aKaMSh7kSpVSZ6pwNXMfAP5dRG4H3gVKgd4nPOlGRJYDywHGjRsXppK6aaqA\nt34MExYMucA/XNPCMx8U8/ymwzS2exmV4mLiyCRuPi+DiSMSmTwqmZljUsI2c6JSavgKJfRLgbFB\nj3MCyzoZY8qwLX1EJBG4zhhTJyKlwKXdtn2n+wsYY54GngY7y2bo5ffDm4+CpwWu/pchEfh1LW7e\n3VvFq9vLeKPoKCLC1TNGccf8XPLHpQ12eUqpCBVK6G8CJopIHjbslwLLglcQkUygxhjjB76LHckD\n8DrwExHpSLErA8+fXoc/tvPozP8OZE48bS/r9flpCEzNW9fipq7Vw7bDdfz3Z5VsO1yH39gzV//h\nkrP46vnjGZUSd9pqU0oNT32GvjHGKyL3YAPcCaw2xuwUkZVAoTFmA7Y1/1MRMdjunW8Gtq0RkR9i\nvzgAVnYc1D1t/D549X5IHgMXP3jaXnb9psP871c+pd3bdd4ZEZiVk8q9l03kkklZzMpJ1QOvSqnT\nJqQ+fWPMRmBjt2UPB91/AXihl21Xc6zlf/oVrrZn3V6/5rQMzTTG8B/v7OdfXt/D+RMyuHLaSFLi\nokmNjyYlLpq8TDupmFJKDYahO14xHJqr4K0f2itbTVtyyl/O7zf88NUi1vytmMWzR/Mv18/Sk56U\nUkNKZIf+354Ad/NpOXjr9vp54I/b2LCtjDvn5/HQl6boaBul1JAT2aH/+Ucw9jx7/dow21FSz86y\nevYHxtPvPtJIeX0b/2vRJO6+5CydYVIpNSRFbuj7fXD0U8j/ath3/fS7+/nJxt2AvQB2XmYCc8en\ncfWMUVw9Y1TYX08ppcIlckO/er8dl589M6y7/dP2Mn6ycTdfmjGKFV+czOjUOB19o5Q6Y0Ru6B/Z\nbn+OCl/of3ywhvuf38a5uWn8642zdCIzpdQZJ3KHlhzZDo5oyAxPf/7+yia+/mwhOelx/OarBRr4\nSqkzUuSGfvl2GDEFogY+Jr6ysZ3b13xMtFN45o55pMbrOHul1JkpMkPfGHtCVhi6dupbPdy+5mOq\nGt2svv1cxqbHh6FApZQaHJEZ+o3l0FI14IO4ze1e7ljzMZ8dbeSpW/KZmZMapgKVUmpwROaB3PLA\nQdwBhH6bx8ddzxSyraSeVcvyuXTSiDAVp5RSgycyW/pHdtif2dNPanO318/d/28zHx6s5l9vmMWi\n6Wf29XOVUqpDhIb+NkifALFJ/d7U6/Pz7XVbeHtPJT9ZMoMvzxm8C30ppVS4RWbol28/qa4dr8/P\nfeu38dqnR3joS1O4ad4puoqXUkoNksgL/dY6qDvU75E7Pr/hgT9u47+2lbHii5O566IJp6hApZQa\nPJEX+kc/tT/70dL3+Q0PvrCNl7eW8eBVk/jGJWedouKUUmpwRV7o93Pkjt9vWPHidv7zk1LuX3gO\n31xw9iksTimlBlfkhf6RHZA4EpJGhrT6E29+xh83l/DtyyfyrctP3/VzlVJqMERg6G+H7Bkhrer2\n+nnuw0NcNW0k37lCA18pFflCCn0RWSQie0Rkn4is6OH5cSLytohsEZHtInJ1YHmuiLSKyNbA7Vfh\nfgNdeNuhcnfIXTtv7a6gtsXD0nnj9KInSqlhoc8zckXECawCFgIlwCYR2WCMKQpa7SFgvTHmKRGZ\nir2Iem7guf3GmNnhLbsXFbvA7w155M6Ln5SQlRTLRWdnnuLClFJqaAilpT8P2GeMOWCMcQPrgMXd\n1jFAcuB+ClAWvhL74UjoB3Grm9p5e3cFS+aMIcoZeb1cSinVk1DSbgxwOOhxSWBZsEeBW0SkBNvK\nvzfoubxAt89/i8hFAym2T+XbISYJ0vL6XHXDtjK8fsN1+TmntCSllBpKwtXEvQn4nTEmB7gaeE5E\nHEA5MM4YMwe4H/iDiCR331hElotIoYgUVlZWnnwVR3bY+XYcfb+tFzaXMGNMCpOy+z9Vg1JKnalC\nCf1SYGzQ45zAsmBfA9YDGGP+DriATGNMuzGmOrB8M7AfOKf7CxhjnjbGFBhjCrKysvr/LgD8fnti\nVggjd3YfaWBnWQPX5eu8Okqp4SWU0N8ETBSRPBGJAZYCG7qt8zlwOYCITMGGfqWIZAUOBCMiE4CJ\nwIFwFd9FYxn4fSH157+4uYRop3DNbA19pdTw0ufoHWOMV0TuAV4HnMBqY8xOEVkJFBpjNgD/BPxG\nRO7DHtS93RhjRORiYKWIeAA/8A1jTM0peScpOfC9Ujt65wS8Pj8vbSljwaQRpCfoZQ+VUsNLSBdR\nMcZsxB6gDV72cND9ImB+D9u9CLw4wBpD53Da2wm8t7eKqqZ2rpurB3CVUsPPsBur+MInJaTFR7NA\nr4SllBqGhlXoN7R5eKPoKItnjyEmali9daWUAoZZ6BcW1+D2+rlqml7+UCk1PA2r0N9UXEu0U5g9\nNnWwS1FKqUExrEK/sLiG6WNSiIs58cFepZSKVMMm9Ns8PrYdrufc3PTBLkUppQbNsAn9T0vrcfv8\nFIxPG+xSlFJq0Ayb0N9UXAvAXA19pdQwNmxCv7C4hrOyEshIjB3sUpRSatAMi9D3+w2Fh2q1P18p\nNewNi9DfW9FEfauHAg19pdQwNyxCf1OxnePt3Fztz1dKDW/DIvQLi2vISoplXHr8YJeilFKDaliE\n/qbiWublpiMig12KUkoNqogP/bK6VkrrWinQrh2llIr80C88ZMfn68gdpZQaDqFfXENCjJPJegF0\npZSK/NDfVFxL/vg0opwR/1aVUqpPEZ2EDW0edh9poGC8du0opRSEGPoiskhE9ojIPhFZ0cPz40Tk\nbRHZIiLbReTqoOe+G9huj4hcFc7i+7L5UC3G6Ph8pZTq0OeF0UXECawCFgIlwCYR2RC4GHqHh4D1\nxpinRGQq9iLquYH7S4FpwGjgTRE5xxjjC/cb6cnm4lqcDmH2OL1oilJKQWgt/XnAPmPMAWOMG1gH\nLO62jgGSA/dTgLLA/cXAOmNMuzHmILAvsL/Toqy+lexkF/ExfX63KaXUsBBK6I8BDgc9LgksC/Yo\ncIuIlGBb+ff2Y9tTps3jI16vkqWUUp3CdSD3JuB3xpgc4GrgOREJed8islxECkWksLKyMkwlQavb\np5dGVEqpIKEEcykwNuhxTmBZsK8B6wGMMX8HXEBmiNtijHnaGFNgjCnIysoKvfo+tHp8uKI19JVS\nqkMoob8JmCgieSISgz0wu6HbOp8DlwOIyBRs6FcG1lsqIrEikgdMBD4OV/F9afX4idPQV0qpTn0e\n4TTGeEXkHuB1wAmsNsbsFJGVQKExZgPwT8BvROQ+7EHd240xBtgpIuuBIsALfPN0jdwBaHP7iEt2\nna6XU0qpIS+kYS3GmI3YA7TByx4Oul8EzO9l2x8DPx5AjSet1aN9+kopFSyiz8jVPn2llOoqokO/\nze3TPn2llAoS0aFvu3ci+i0qpVS/RGwienx+vH6DK0pb+kop1SFiQ7/VYwcJ6YFcpZQ6JmJDv81t\nQ18P5Cql1DERG/qdLX0NfaWU6hT5oa/dO0op1SlyQ9+tLX2llOouckPfo336SinVXcSGfpt27yil\n1HEiOPT9gHbvKKVUsIgNfe3TV0qp40Vu6Hf06es0DEop1SliE7FNx+krpdRxIjb0W/WMXKWUOk7k\nhr7HR7RTiHZG7FtUSql+i9hE1AuoKKXU8SI29Ns8egEVpZTqLqTQF5FFIrJHRPaJyIoenv83Edka\nuH0mInVBz/mCntsQzuJPpNWt18dVSqnu+rwwuog4gVXAQqAE2CQiGwIXQwfAGHNf0Pr3AnOCdtFq\njJkdvpJD06otfaWUOk4oLf15wD5jzAFjjBtYByw+wfo3AWvDUdxAtHr82qevlFLdhBL6Y4DDQY9L\nAsuOIyLjgTzgraDFLhEpFJEPReTLJ11pP+lF0ZVS6nh9du/001LgBWOML2jZeGNMqYhMAN4SkR3G\nmP3BG4nIcmA5wLhx48JSSKvHR1ZSbFj2pZRSkSKUln4pMDbocU5gWU+W0q1rxxhTGvh5AHiHrv39\nHes8bYwpMMYUZGVlhVBS37RPXymljhdK6G8CJopInojEYIP9uFE4IjIZSAP+HrQsTURiA/czgflA\nUfdtT4VWt47TV0qp7vrs3jHGeEXkHuB1wAmsNsbsFJGVQKExpuMLYCmwzhhjgjafAvxaRPzYL5jH\ngkf9nEptHh9xOtmaUkp1EVKfvjFmI7Cx27KHuz1+tIftPgBmDKC+k6bdO0opdbyIbAobYzT0lVKq\nBxEZ+u1eP8aAS8/IVUqpLiIy9HUufaWU6llEhn6rhr5SSvUoMkO/4/q42r2jlFJdRGboe/SqWUop\n1ZOIDH3t01dKqZ5FZOi3uv2Adu8opVR3kRn62tJXSqkeRXToa5++Ukp1FZGh36ajd5RSqkcRGfqd\nLf2oiHx7Sil10iIyFTtH72hLXymluojI0D/W0tfQV0qpYBEb+rFRDhwOGexSlFJqSInI0G9z+7Rr\nRymlehCRoa9z6SulVM8iNPT9GvpKKdWDyAx9vSi6Ukr1KKTQF5FFIrJHRPaJyIoenv83EdkauH0m\nInVBz90mInsDt9vCWXxv7EXRNfSVUqq7Pi+MLiJOYBWwECgBNonIBmNMUcc6xpj7gta/F5gTuJ8O\nPAIUAAbYHNi2Nqzvohvt01dKqZ6F0tKfB+wzxhwwxriBdcDiE6x/E7A2cP8q4A1jTE0g6N8AFg2k\n4FBo945SSvUslNAfAxwOelx32IyVAAAWQElEQVQSWHYcERkP5AFv9XfbcNLuHaWU6lm4D+QuBV4w\nxvj6s5GILBeRQhEprKysHHARtnsnIo9RK6XUgISSjKXA2KDHOYFlPVnKsa6dkLc1xjxtjCkwxhRk\nZWWFUNKJaZ++Ukr1LJTQ3wRMFJE8EYnBBvuG7iuJyGQgDfh70OLXgStFJE1E0oArA8tOqVa3D5d2\n7yil1HH6HL1jjPGKyD3YsHYCq40xO0VkJVBojOn4AlgKrDPGmKBta0Tkh9gvDoCVxpia8L6Frvx+\nQ7tXT85SSqme9Bn6AMaYjcDGbsse7vb40V62XQ2sPsn6+q3Nq5dKVEqp3kTc0c5WvWqWUkr1KvJC\nX6+Pq5RSvYq40O+8apaGvlJKHSfiQr/V7Qc09JVSqieRF/p6fVyllOpVxIa+9ukrpdTxIi/03dqn\nr5RSvYm40G/T7h2llOpVxIV+q47eUUqpXkVe6Gv3jlJK9SryQr/jQG5MxL01pZQasIhLxjaPD4dA\njDPi3ppSSg1YxCVjq9vOpS8ig12KUkoNOREX+m1evVSiUkr1JuJCv9Xt1xOzlFKqFxEX+m16qUSl\nlOpVxIV+q0e7d5RSqjeRF/pun3bvKKVULyIv9D0a+kop1ZuQQl9EFonIHhHZJyIrelnnRhEpEpGd\nIvKHoOU+EdkauG3oadtwsn36EfddppRSYdHnhdFFxAmsAhYCJcAmEdlgjCkKWmci8F1gvjGmVkRG\nBO2i1RgzO8x196pVD+QqpVSvQmkSzwP2GWMOGGPcwDpgcbd1vg6sMsbUAhhjKsJbZuha3XogVyml\nehNK6I8BDgc9LgksC3YOcI6I/E1EPhSRRUHPuUSkMLD8ywOst0/ap6+UUr3rs3unH/uZCFwK5ADv\nisgMY0wdMN4YUyoiE4C3RGSHMWZ/8MYishxYDjBu3LgBFaLj9JVSqnehhH4pMDbocU5gWbAS4CNj\njAc4KCKfYb8ENhljSgGMMQdE5B1gDtAl9I0xTwNPAxQUFJiTeB8AeHx+PD6joa8igsfjoaSkhLa2\ntsEuRQ0hLpeLnJwcoqOjT2r7UEJ/EzBRRPKwYb8UWNZtnZeBm4A1IpKJ7e45ICJpQIsxpj2wfD7w\nf06q0hDoVbNUJCkpKSEpKYnc3FydQFABYIyhurqakpIS8vLyTmofffbpG2O8wD3A68AuYL0xZqeI\nrBSRawKrvQ5Ui0gR8DbwoDGmGpgCFIrItsDyx4JH/YSbXhRdRZK2tjYyMjI08FUnESEjI2NAf/2F\n1KdvjNkIbOy27OGg+wa4P3ALXucDYMZJV9dPbW4/oFfNUpFDA191N9DfiYg6i6lVu3eUCpvq6mpm\nz57N7Nmzyc7OZsyYMZ2P3W53SPu444472LNnzwnXWbVqFb///e/DUTIAR48eJSoqit/+9rdh22ck\nCdfonSFBL4quVPhkZGSwdetWAB599FESExN54IEHuqxjjMEYg8PRc/txzZo1fb7ON7/5zYEXG2T9\n+vWcf/75rF27lrvuuius+w7m9XqJijrzIjSyWvpu7dNX6lTbt28fU6dO5eabb2batGmUl5ezfPly\nCgoKmDZtGitXruxc98ILL2Tr1q14vV5SU1NZsWIFs2bN4vzzz6eiwp7D+dBDD/HEE090rr9ixQrm\nzZvHpEmT+OCDDwBobm7muuuuY+rUqVx//fUUFBR0fiF1t3btWp544gkOHDhAeXl55/JXX32V/Px8\nZs2axZVXXglAY2Mjt912GzNnzmTmzJm8/PLLnbV2WLduXeeXxy233MLdd9/NvHnz+N73vseHH37I\n+eefz5w5c5g/fz579+4F7BfCfffdx/Tp05k5cyb/8R//wV/+8heuv/76zv2+9tpr3HDDDQP+9+iv\nM+9r6gR09I6KVD/4r50UlTWEdZ9TRyfzyP+cdlLb7t69m2effZaCggIAHnvsMdLT0/F6vSxYsIDr\nr7+eqVOndtmmvr6eSy65hMcee4z777+f1atXs2LF8VN5GWP4+OOP2bBhAytXruTPf/4zTz75JNnZ\n2bz44ots27aN/Pz8HusqLi6mpqaGuXPncsMNN7B+/Xq+/e1vc+TIEe6++27ee+89xo8fT01NDWD/\ngsnKymL79u0YY6irq+vzvZeXl/Phhx/icDior6/nvffeIyoqij//+c889NBDPP/88zz11FOUlZWx\nbds2nE4nNTU1pKamcs8991BdXU1GRgZr1qzhzjvv7O9HP2CR1dLX7h2lTouzzjqrM/DBtq7z8/PJ\nz89n165dFBUdP0gvLi6OL37xiwDMnTuX4uLiHvd97bXXHrfO+++/z9KlSwGYNWsW06b1/GW1bt06\nvvKVrwCwdOlS1q5dC8Df//53FixYwPjx4wFIT08H4M033+zsXhIR0tLS+nzvN9xwQ2d3Vl1dHddd\ndx3Tp0/ngQceYOfOnZ37/cY3voHT6ex8PYfDwc0338wf/vAHampq2Lx5c+dfHKdTRLX0O7p3NPRV\npDnZFvmpkpCQ0Hl/7969/OIXv+Djjz8mNTWVW265pcchhTExMZ33nU4nXq+3x33Hxsb2uU5v1q5d\nS1VVFc888wwAZWVlHDhwoF/7cDgc2AGJVvf3Evzev//973PVVVfxj//4j+zbt49FixZxInfeeSfX\nXXcdAF/5ylc6vxROp4hs6btiIuptKTWkNTQ0kJSURHJyMuXl5bz++uthf4358+ezfv16AHbs2NHj\nXxJFRUV4vV5KS0spLi6muLiYBx98kHXr1nHBBRfw9ttvc+jQIYDO7p2FCxeyatUqwHYr1dbW4nA4\nSEtLY+/evfj9fl566aVe66qvr2fMGDsV2e9+97vO5QsXLuRXv/oVPp+vy+uNHTuWzMxMHnvsMW6/\n/faBfSgnKaLSsU27d5Q67fLz85k6dSqTJ0/mq1/9KvPnzw/7a9x7772UlpYydepUfvCDHzB16lRS\nUlK6rLN27VqWLFnSZdl1113H2rVrGTlyJE899RSLFy9m1qxZ3HzzzQA88sgjHD16lOnTpzN79mze\ne+89AH72s59x1VVXccEFF5CTk9NrXf/8z//Mgw8+SH5+fpe/Dv7hH/6B7OxsZs6cyaxZszq/sACW\nLVtGXl4e55xzzoA/l5MhwYUOBQUFBaawsPCktn3yr3v51zc+Y++Pv0i0M6K+z9QwtGvXLqZMmTLY\nZQwJXq8Xr9eLy+Vi7969XHnllezdu/eMHDL5jW98g/PPP5/bbrvtpPfR0++GiGw2xhT0skmnM+8T\nO4FWj49op2jgKxVhmpqauPzyy/F6vRhj+PWvf31GBv7s2bNJS0vjl7/85aDVcOZ9aiegc+krFZlS\nU1PZvHnzYJcxYL2dW3A6RVSTWOfSV0qpE4uo0NdLJSql1IlFVuhrS18ppU4ookK/zePXPn2llDqB\niAp9bekrFT4LFiw47kSrJ554grvvvvuE2yUmJgL2bNjgCcaCXXrppfQ1NPuJJ56gpaWl8/HVV18d\n0tw4oZo9e3bn1A7DSUSFfptH+/SVCpebbrqJdevWdVm2bt06brrpppC2Hz16NC+88MJJv3730N+4\ncWOX2S8HYteuXfh8Pt577z2am5vDss+e9HcaidMhokK/1a0tfaXC5frrr+fVV1/tvGBKcXExZWVl\nXHTRRZ3j5vPz85kxYwavvPLKcdsXFxczffp0AFpbW1m6dClTpkxhyZIltLa2dq539913d07L/Mgj\njwDwy1/+krKyMhYsWMCCBQsAyM3NpaqqCoCf//znTJ8+nenTp3dOy1xcXMyUKVP4+te/zrRp07jy\nyiu7vE6wtWvXcuutt3LllVd2qX3fvn1cccUVzJo1i/z8fPbv3w/YM3RnzJjBrFmzOmcGDf5rpaqq\nitzcXMBOx3DNNddw2WWXcfnll5/ws3r22Wc7z9q99dZbaWxsJC8vD4/HA9gpLoIfh4OO01fqTPDa\nCjiyI7z7zJ4BX3ys16fT09OZN28er732GosXL2bdunXceOONiAgul4uXXnqJ5ORkqqqq+MIXvsA1\n11zT66X8nnrqKeLj49m1axfbt2/vMjXyj3/8Y9LT0/H5fFx++eVs376db33rW/z85z/n7bffJjMz\ns8u+Nm/ezJo1a/joo48wxnDeeedxySWXdM6Xs3btWn7zm99w44038uKLL3LLLbccV8/zzz/PG2+8\nwe7du3nyySdZtmwZADfffDMrVqxgyZIltLW14ff7ee2113jllVf46KOPiI+P75xH50Q++eQTtm/f\n3jnddE+fVVFRET/60Y/44IMPyMzMpKamhqSkJC699FJeffVVvvzlL7Nu3TquvfZaoqOj+3zNUIXU\n0heRRSKyR0T2icjxE2DbdW4UkSIR2SkifwhafpuI7A3cTv684xDY7p2I+uNFqUEV3MUT3LVjjOF7\n3/seM2fO5IorrqC0tJSjR4/2up933323M3w7LljSYf369eTn5zNnzhx27tzZ42Rqwd5//32WLFlC\nQkICiYmJXHvttZ1z5uTl5TF79myg9+mbCwsLyczMZNy4cVx++eVs2bKFmpoaGhsbKS0t7Zy/x+Vy\nER8fz5tvvskdd9xBfHw8cGxa5hNZuHBh53q9fVZvvfUWN9xwQ+eXWsf6d911V+cVx9asWcMdd9zR\n5+v1R58tfRFxAquAhUAJsElENhhjioLWmQh8F5hvjKkVkRGB5enAI0ABYIDNgW1rw/ouArR7R0Ws\nE7TIT6XFixdz33338cknn9DS0sLcuXMB+P3vf09lZSWbN28mOjqa3NzcHqdT7svBgwd5/PHH2bRp\nE2lpadx+++0ntZ8OHdMyg52auafunbVr17J79+7O7piGhgZefPHFfh/UjYqKwu/3Ayeefrm/n9X8\n+fMpLi7mnXfewefzdXaRhUsozeJ5wD5jzAFjjBtYByzuts7XgVUdYW6MqQgsvwp4wxhTE3juDeDE\nE06fJGOMjt5RKswSExNZsGABd955Z5cDuPX19YwYMYLo6OguUxb35uKLL+YPf7AdAJ9++inbt28H\nbOAmJCSQkpLC0aNHee211zq3SUpKorGx8bh9XXTRRbz88su0tLTQ3NzMSy+9xEUXXRTS+/H7/axf\nv54dO3Z0Tr/8yiuvsHbtWpKSksjJyeHll18GoL29nZaWFhYuXMiaNWs6Dyp3dO/k5uZ2Tg1xogPW\nvX1Wl112GX/84x+prq7usl+Ar371qyxbtizsrXwILfTHAIeDHpcElgU7BzhHRP4mIh+KyKJ+bBsW\nbp8fvwGXjt5RKqxuuukmtm3b1iX0b775ZgoLC5kxYwbPPvsskydPPuE+7r77bpqampgyZQoPP/xw\n518Ms2bNYs6cOUyePJlly5Z1mZZ5+fLlLFq0qPNAbof8/Hxuv/125s2bx3nnncddd93FnDlzQnov\n7733HmPGjGH06NGdyy6++GKKioooLy/nueee45e//CUzZ87kggsu4MiRIyxatIhrrrmGgoICZs+e\nzeOPPw7AAw88wFNPPcWcOXM6DzD3pLfPatq0aXz/+9/nkksuYdasWdx///1dtqmtrQ15pFR/9Dm1\nsohcDywyxtwVeHwrcJ4x5p6gdf4EeIAbgRzgXWAGcBfgMsb8KLDe/wZajTGPd3uN5cBygHHjxs3t\nq9XQk/oWD7NW/oVH/udU7pif1+/tlRpqdGrl4euFF17glVde4bnnnuvx+VM9tXIpMDbocU5gWbAS\n4CNjjAc4KCKfARMD613abdt3ur+AMeZp4Gmw8+mHUNPxBL40cxQTshJPanOllBoK7r33Xl577TU2\nbtx4SvYfSuhvAiaKSB42xJcCy7qt8zJwE7BGRDKx3T0HgP3AT0Sk42rDV2IP+IZdSlw0q5bl972i\nUkoNYU8++eQp3X+foW+M8YrIPcDrgBNYbYzZKSIrgUJjzIbAc1eKSBHgAx40xlQDiMgPsV8cACuN\nMX0PclVKKXVKhHRyljFmI7Cx27KHg+4b4P7Arfu2q4HVAytTqeHJGNPrCU9qeBroJW71TCalhiiX\ny0V1dfWA/5OryGGMobq6GpfLddL7iKhpGJSKJDk5OZSUlFBZWTnYpaghxOVykZOTc9Lba+grNURF\nR0eTl6fDj1V4afeOUkoNIxr6Sik1jGjoK6XUMNLnNAynm4hUAv2fh+GYTKD3iTCGnjOtXtCaT5cz\nreYzrV6IrJrHG2Oy+tp4yIX+QIlIYSjzTwwVZ1q9oDWfLmdazWdavTA8a9buHaWUGkY09JVSahiJ\nxNB/erAL6KczrV7Qmk+XM63mM61eGIY1R1yfvlJKqd5FYktfKaVULyIm9EVkkYjsEZF9IrJisOvp\niYisFpEKEfk0aFm6iLwhInsDP9NOtI/TTUTGisjbIlIkIjtF5NuB5UOybhFxicjHIrItUO8PAsvz\nROSjwO/H8yISM9i1diciThHZErgS3ZCvWUSKRWSHiGwVkcLAsiH5e9FBRFJF5AUR2S0iu0Tk/KFa\ns4hMCny2HbcGEfnOQOuNiNAXESewCvgiMBW4SUSmDm5VPfodx18YfgXwV2PMROCvgcdDiRf4J2PM\nVOALwDcDn+1QrbsduMwYMwuYDSwSkS8APwP+zRhzNlALfG0Qa+zNt4FdQY/PhJoXGGNmBw0hHKq/\nFx1+AfzZGDMZmIX9vIdkzcaYPYHPdjYwF2gBXmKg9RpjzvgbcD7wetDj7wLfHey6eqk1F/g06PEe\nYFTg/ihgz2DX2Ef9rwALz4S6gXjgE+A87MksUT39vgyFG/ZSon8FLgP+BMgZUHMxkNlt2ZD9vQBS\ngIMEjmWeCTUH1Xgl8Ldw1BsRLX1gDHA46HFJYNmZYKQxpjxw/wgwcjCLORERyQXmAB8xhOsOdJNs\nBSqAN7CX7awzxngDqwzF348ngP8F+AOPMxj6NRvgLyKyWUSWB5YN2d8LIA+oxF7WdYuI/FZEEhja\nNXdYCqwN3B9QvZES+hHB2K/uITmcSkQSgReB7xhjGoKfG2p1G2N8xv5JnAPMAyYPckknJCL/A6gw\nxmwe7Fr66UJjTD62W/WbInJx8JND7fcCO5V8PvCUMWYO0Ey3rpEhWDOBYznXAH/s/tzJ1BspoV8K\njA16nBNYdiY4KiKjAAI/Kwa5nuOISDQ28H9vjPnPwOIhX7cxpg54G9s1kioiHdePGGq/H/OBa0Sk\nGFiH7eL5BUO7ZowxpYGfFdi+5nkM7d+LEqDEGPNR4PEL2C+BoVwz2C/VT4wxRwOPB1RvpIT+JmBi\nYLRDDPZPoQ2DXFOoNgC3Be7fhu0zHzLEXqD1/wK7jDE/D3pqSNYtIlkikhq4H4c9/rALG/7XB1Yb\nMvUCGGO+a4zJMcbkYn933zLG3MwQrllEEkQkqeM+ts/5U4bo7wWAMeYIcFhEJgUWXQ4UMYRrDriJ\nY107MNB6B/sARRgPdFwNfIbtv/3+YNfTS41rgXLAg211fA3bd/tXYC/wJpA+2HV2q/lC7J+P24Gt\ngdvVQ7VuYCawJVDvp8DDgeUTgI+Bfdg/k2MHu9Ze6r8U+NNQrzlQ27bAbWfH/7mh+nsRVPdsoDDw\n+/EykDaUawYSgGogJWjZgOrVM3KVUmoYiZTuHaWUUiHQ0FdKqWFEQ18ppYYRDX2llBpGNPSVUmoY\n0dBXSqlhRENfKaWGEQ19pZQaRv4/yyEO1+oIVF0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}