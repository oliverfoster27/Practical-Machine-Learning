{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C7 Exercises.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oliverfoster27/Practical-Machine-Learning/blob/master/Week%207/C7_Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ywZ2fsAxP6ON",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l94hdaVKQTtF",
        "colab_type": "code",
        "outputId": "c8c79a27-069f-49a3-934b-14b0660cf4e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data('/tmp/mnist.npz')\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "X_train = X_train.reshape(-1, 28*28)\n",
        "X_test = X_test.reshape(-1, 28*28)\n",
        "X_train.shape, X_test.shape, y_train, y_test"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 784),\n",
              " (10000, 784),\n",
              " array([5, 0, 4, ..., 5, 6, 8], dtype=uint8),\n",
              " array([7, 2, 1, ..., 4, 5, 6], dtype=uint8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "Xj0EkNVnxzhl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255\n",
        "X_test = X_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sORyfkzHQYRQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "Make a dense neural network with 95%+ accuracy on Mnist that has the smallest number of neurons possible by experimenting with Dropout, and Batch Norm"
      ]
    },
    {
      "metadata": {
        "id": "xLeSJuaF34gr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_grid = [10e-4, 10e-5, 10e-6, 10e-7, 10e-8]\n",
        "input_dense_grid = np.linspace(10, 100, 19).astype(int)\n",
        "p_grid = [0.2, 0.4, 0.6, 0.8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t7S3iZx1QWrB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "def find_minimal_network(features, output):\n",
        "\n",
        "  data = []\n",
        "\n",
        "  for input_dense in input_dense_grid:\n",
        "\n",
        "    for p in p_grid:\n",
        "\n",
        "      for learning_rate in learning_grid:\n",
        "\n",
        "        K.clear_session()\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(input_dense, activation='relu', input_shape=(784,)))\n",
        "        model.add(Dropout(p))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "        callback_list = [EarlyStopping(monitor='val_acc', mode='max',\n",
        "                                       verbose=1, patience=10)]\n",
        "\n",
        "        optimizer = Adam(lr=learning_rate)\n",
        "        model.compile(optimizer=optimizer,\n",
        "                     loss='sparse_categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "        print(model.summary())\n",
        "\n",
        "        h = model.fit(features, output, epochs=500, validation_split=0.3,\n",
        "                     callbacks=callback_list)\n",
        "\n",
        "        if h.history['acc'][-1] >= 0.95:\n",
        "          res={\n",
        "            'Input Dense Layer Size': input_dense,\n",
        "            'Learning Rate': learning_rate,\n",
        "            'Dropout Proportion': p,\n",
        "            'Validation Accuracy': h.history['acc'][-1]\n",
        "          }\n",
        "          # If we've satisfied 95% accuracy leave the learning rate grid\n",
        "          return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r4ddfMEDzDSx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4970
        },
        "outputId": "3306aa15-ca9b-42e7-ff9e-f6d4fb404b83"
      },
      "cell_type": "code",
      "source": [
        "find_minimal_network(X_train, y_train)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 10)                40        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 8,000\n",
            "Trainable params: 7,980\n",
            "Non-trainable params: 20\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/500\n",
            "42000/42000 [==============================] - 3s 82us/step - loss: 2.6639 - acc: 0.0968 - val_loss: 2.5783 - val_acc: 0.0961\n",
            "Epoch 2/500\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 2.6613 - acc: 0.0986 - val_loss: 2.5660 - val_acc: 0.0959\n",
            "Epoch 3/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.6533 - acc: 0.0997 - val_loss: 2.5635 - val_acc: 0.0971\n",
            "Epoch 4/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.6487 - acc: 0.0993 - val_loss: 2.5628 - val_acc: 0.0984\n",
            "Epoch 5/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.6428 - acc: 0.1013 - val_loss: 2.5497 - val_acc: 0.1002\n",
            "Epoch 6/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.6353 - acc: 0.1002 - val_loss: 2.5465 - val_acc: 0.1013\n",
            "Epoch 7/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.6298 - acc: 0.1039 - val_loss: 2.5396 - val_acc: 0.1039\n",
            "Epoch 8/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.6274 - acc: 0.1018 - val_loss: 2.5357 - val_acc: 0.1035\n",
            "Epoch 9/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.6201 - acc: 0.1038 - val_loss: 2.5312 - val_acc: 0.1056\n",
            "Epoch 10/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.6174 - acc: 0.1067 - val_loss: 2.5222 - val_acc: 0.1074\n",
            "Epoch 11/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.6099 - acc: 0.1070 - val_loss: 2.5191 - val_acc: 0.1093\n",
            "Epoch 12/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.6060 - acc: 0.1069 - val_loss: 2.5146 - val_acc: 0.1091\n",
            "Epoch 13/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.6029 - acc: 0.1081 - val_loss: 2.5149 - val_acc: 0.1109\n",
            "Epoch 14/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.5930 - acc: 0.1097 - val_loss: 2.5008 - val_acc: 0.1128\n",
            "Epoch 15/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.5879 - acc: 0.1126 - val_loss: 2.5016 - val_acc: 0.1135\n",
            "Epoch 16/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.5841 - acc: 0.1112 - val_loss: 2.4992 - val_acc: 0.1142\n",
            "Epoch 17/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.5780 - acc: 0.1126 - val_loss: 2.4846 - val_acc: 0.1173\n",
            "Epoch 18/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.5771 - acc: 0.1138 - val_loss: 2.4863 - val_acc: 0.1174\n",
            "Epoch 19/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.5668 - acc: 0.1161 - val_loss: 2.4785 - val_acc: 0.1185\n",
            "Epoch 20/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.5629 - acc: 0.1156 - val_loss: 2.4837 - val_acc: 0.1192\n",
            "Epoch 21/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.5551 - acc: 0.1189 - val_loss: 2.4737 - val_acc: 0.1214\n",
            "Epoch 22/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.5592 - acc: 0.1167 - val_loss: 2.4667 - val_acc: 0.1228\n",
            "Epoch 23/500\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 2.5457 - acc: 0.1207 - val_loss: 2.4645 - val_acc: 0.1239\n",
            "Epoch 24/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.5453 - acc: 0.1218 - val_loss: 2.4600 - val_acc: 0.1249\n",
            "Epoch 25/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.5374 - acc: 0.1215 - val_loss: 2.4516 - val_acc: 0.1274\n",
            "Epoch 26/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.5333 - acc: 0.1239 - val_loss: 2.4474 - val_acc: 0.1287\n",
            "Epoch 27/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.5263 - acc: 0.1239 - val_loss: 2.4371 - val_acc: 0.1299\n",
            "Epoch 28/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.5229 - acc: 0.1243 - val_loss: 2.4283 - val_acc: 0.1321\n",
            "Epoch 29/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.5160 - acc: 0.1270 - val_loss: 2.4331 - val_acc: 0.1324\n",
            "Epoch 30/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.5172 - acc: 0.1267 - val_loss: 2.4232 - val_acc: 0.1337\n",
            "Epoch 31/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.5019 - acc: 0.1311 - val_loss: 2.4158 - val_acc: 0.1362\n",
            "Epoch 32/500\n",
            "42000/42000 [==============================] - 3s 69us/step - loss: 2.5009 - acc: 0.1282 - val_loss: 2.4156 - val_acc: 0.1366\n",
            "Epoch 33/500\n",
            "42000/42000 [==============================] - 3s 68us/step - loss: 2.5004 - acc: 0.1269 - val_loss: 2.4109 - val_acc: 0.1389\n",
            "Epoch 34/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.4935 - acc: 0.1335 - val_loss: 2.4041 - val_acc: 0.1400\n",
            "Epoch 35/500\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 2.4855 - acc: 0.1318 - val_loss: 2.4033 - val_acc: 0.1409\n",
            "Epoch 36/500\n",
            "42000/42000 [==============================] - 3s 69us/step - loss: 2.4835 - acc: 0.1330 - val_loss: 2.3975 - val_acc: 0.1423\n",
            "Epoch 37/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.4800 - acc: 0.1371 - val_loss: 2.3934 - val_acc: 0.1443\n",
            "Epoch 38/500\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 2.4708 - acc: 0.1375 - val_loss: 2.3858 - val_acc: 0.1447\n",
            "Epoch 39/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.4672 - acc: 0.1377 - val_loss: 2.3810 - val_acc: 0.1476\n",
            "Epoch 40/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.4648 - acc: 0.1377 - val_loss: 2.3795 - val_acc: 0.1493\n",
            "Epoch 41/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.4575 - acc: 0.1405 - val_loss: 2.3750 - val_acc: 0.1501\n",
            "Epoch 42/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.4574 - acc: 0.1413 - val_loss: 2.3618 - val_acc: 0.1521\n",
            "Epoch 43/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.4482 - acc: 0.1417 - val_loss: 2.3600 - val_acc: 0.1534\n",
            "Epoch 44/500\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 2.4406 - acc: 0.1429 - val_loss: 2.3609 - val_acc: 0.1550\n",
            "Epoch 45/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.4370 - acc: 0.1447 - val_loss: 2.3616 - val_acc: 0.1564\n",
            "Epoch 46/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.4368 - acc: 0.1465 - val_loss: 2.3493 - val_acc: 0.1578\n",
            "Epoch 47/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.4290 - acc: 0.1489 - val_loss: 2.3452 - val_acc: 0.1611\n",
            "Epoch 48/500\n",
            "42000/42000 [==============================] - 3s 67us/step - loss: 2.4255 - acc: 0.1473 - val_loss: 2.3496 - val_acc: 0.1608\n",
            "Epoch 49/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.4191 - acc: 0.1508 - val_loss: 2.3319 - val_acc: 0.1631\n",
            "Epoch 50/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.4137 - acc: 0.1491 - val_loss: 2.3276 - val_acc: 0.1656\n",
            "Epoch 51/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.4080 - acc: 0.1547 - val_loss: 2.3236 - val_acc: 0.1657\n",
            "Epoch 52/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.4068 - acc: 0.1525 - val_loss: 2.3212 - val_acc: 0.1688\n",
            "Epoch 53/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.4018 - acc: 0.1536 - val_loss: 2.3162 - val_acc: 0.1687\n",
            "Epoch 54/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.3957 - acc: 0.1566 - val_loss: 2.3127 - val_acc: 0.1715\n",
            "Epoch 55/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.3918 - acc: 0.1553 - val_loss: 2.3108 - val_acc: 0.1733\n",
            "Epoch 56/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.3859 - acc: 0.1563 - val_loss: 2.2991 - val_acc: 0.1752\n",
            "Epoch 57/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.3864 - acc: 0.1617 - val_loss: 2.3011 - val_acc: 0.1765\n",
            "Epoch 58/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.3767 - acc: 0.1610 - val_loss: 2.2947 - val_acc: 0.1783\n",
            "Epoch 59/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.3729 - acc: 0.1600 - val_loss: 2.2852 - val_acc: 0.1799\n",
            "Epoch 60/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.3684 - acc: 0.1633 - val_loss: 2.2831 - val_acc: 0.1814\n",
            "Epoch 61/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.3617 - acc: 0.1643 - val_loss: 2.2804 - val_acc: 0.1832\n",
            "Epoch 62/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.3569 - acc: 0.1664 - val_loss: 2.2736 - val_acc: 0.1856\n",
            "Epoch 63/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.3528 - acc: 0.1672 - val_loss: 2.2715 - val_acc: 0.1871\n",
            "Epoch 64/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.3468 - acc: 0.1670 - val_loss: 2.2763 - val_acc: 0.1872\n",
            "Epoch 65/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.3451 - acc: 0.1709 - val_loss: 2.2619 - val_acc: 0.1914\n",
            "Epoch 66/500\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 2.3364 - acc: 0.1693 - val_loss: 2.2565 - val_acc: 0.1932\n",
            "Epoch 67/500\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: 2.3374 - acc: 0.1697 - val_loss: 2.2506 - val_acc: 0.1942\n",
            "Epoch 68/500\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 2.3318 - acc: 0.1723 - val_loss: 2.2451 - val_acc: 0.1963\n",
            "Epoch 69/500\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 2.3223 - acc: 0.1752 - val_loss: 2.2479 - val_acc: 0.1975\n",
            "Epoch 70/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.3204 - acc: 0.1725 - val_loss: 2.2422 - val_acc: 0.1993\n",
            "Epoch 71/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.3163 - acc: 0.1744 - val_loss: 2.2365 - val_acc: 0.2021\n",
            "Epoch 72/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.3119 - acc: 0.1767 - val_loss: 2.2360 - val_acc: 0.2033\n",
            "Epoch 73/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.3060 - acc: 0.1807 - val_loss: 2.2329 - val_acc: 0.2049\n",
            "Epoch 74/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.3035 - acc: 0.1798 - val_loss: 2.2232 - val_acc: 0.2073\n",
            "Epoch 75/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.3007 - acc: 0.1794 - val_loss: 2.2175 - val_acc: 0.2078\n",
            "Epoch 76/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.2957 - acc: 0.1808 - val_loss: 2.2172 - val_acc: 0.2112\n",
            "Epoch 77/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.2882 - acc: 0.1845 - val_loss: 2.2121 - val_acc: 0.2121\n",
            "Epoch 78/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.2876 - acc: 0.1849 - val_loss: 2.2069 - val_acc: 0.2142\n",
            "Epoch 79/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.2826 - acc: 0.1835 - val_loss: 2.2070 - val_acc: 0.2154\n",
            "Epoch 80/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.2756 - acc: 0.1892 - val_loss: 2.1931 - val_acc: 0.2180\n",
            "Epoch 81/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.2743 - acc: 0.1860 - val_loss: 2.1909 - val_acc: 0.2200\n",
            "Epoch 82/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.2698 - acc: 0.1896 - val_loss: 2.1913 - val_acc: 0.2228\n",
            "Epoch 83/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.2630 - acc: 0.1908 - val_loss: 2.1843 - val_acc: 0.2244\n",
            "Epoch 84/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.2613 - acc: 0.1903 - val_loss: 2.1869 - val_acc: 0.2272\n",
            "Epoch 85/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.2561 - acc: 0.1935 - val_loss: 2.1804 - val_acc: 0.2297\n",
            "Epoch 86/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.2512 - acc: 0.1931 - val_loss: 2.1716 - val_acc: 0.2309\n",
            "Epoch 87/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.2456 - acc: 0.1963 - val_loss: 2.1757 - val_acc: 0.2336\n",
            "Epoch 88/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.2403 - acc: 0.1974 - val_loss: 2.1656 - val_acc: 0.2341\n",
            "Epoch 89/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.2370 - acc: 0.1978 - val_loss: 2.1625 - val_acc: 0.2366\n",
            "Epoch 90/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.2347 - acc: 0.1990 - val_loss: 2.1598 - val_acc: 0.2384\n",
            "Epoch 91/500\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2.2323 - acc: 0.2016 - val_loss: 2.1570 - val_acc: 0.2403\n",
            "Epoch 92/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.2258 - acc: 0.2025 - val_loss: 2.1531 - val_acc: 0.2417\n",
            "Epoch 93/500\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 2.2216 - acc: 0.2031 - val_loss: 2.1507 - val_acc: 0.2428\n",
            "Epoch 94/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.2219 - acc: 0.2042 - val_loss: 2.1450 - val_acc: 0.2447\n",
            "Epoch 95/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.2167 - acc: 0.2055 - val_loss: 2.1414 - val_acc: 0.2471\n",
            "Epoch 96/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.2148 - acc: 0.2061 - val_loss: 2.1342 - val_acc: 0.2477\n",
            "Epoch 97/500\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2.2088 - acc: 0.2075 - val_loss: 2.1296 - val_acc: 0.2496\n",
            "Epoch 98/500\n",
            " 7360/42000 [====>.........................] - ETA: 1s - loss: 2.2034 - acc: 0.2147"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-5f53cc5e5789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfind_minimal_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-65-9f95cf2a22bd>\u001b[0m in \u001b[0;36mfind_minimal_network\u001b[0;34m(features, output)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         h = model.fit(features, output, epochs=500, validation_split=0.3,\n\u001b[0;32m---> 33\u001b[0;31m                      callbacks=callback_list)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_begin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_begin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m     95\u001b[0m            \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.95\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_t_batch\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3495\u001b[0m     \"\"\"\n\u001b[1;32m   3496\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3497\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3403\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msz\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3549\u001b[0m         \u001b[0;31m# warn and return nans like mean would\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3550\u001b[0;31m         \u001b[0mrout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_median_nancheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3552\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3118\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         ret = um.true_divide(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "0GVOW15m0zrR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}